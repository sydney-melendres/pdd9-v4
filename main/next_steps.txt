Based on the nature of the data you're describing - input events over time during an FPS game - and the issues you've encountered with traditional statistical methods, I believe you're dealing with a complex, potentially non-standard distribution. Here are my thoughts and suggestions:

Likely Distribution:
The data is likely to follow a mixture distribution or a zero-inflated distribution. In FPS games, player inputs tend to be bursty (periods of intense activity followed by periods of little to no activity), which doesn't align well with a simple Poisson distribution.
Potential Distributions to Consider:

Zero-Inflated Poisson (ZIP) or Zero-Inflated Negative Binomial (ZINB): These could account for the excess zeros when players are not interacting with inputs.
Hurdle models: These separate the zero and non-zero events, which might be appropriate for your data.
Mixture models: These could capture different "states" of player activity.


Machine Learning Methods to Consider:

Random Forests or Gradient Boosting Machines: These can capture non-linear relationships and handle complex distributions well.
Neural Networks: Particularly recurrent neural networks (RNNs) or LSTMs could capture the temporal nature of the data.
Hidden Markov Models: These could model the transitions between different states of player activity.
Time Series Analysis: Methods like ARIMA or Prophet might be useful if you're interested in forecasting future inputs.


Preprocessing Impact:
The event-focused preprocessing step you provided could indeed make the analysis more insightful and potentially more aligned with a Poisson-like distribution, at least during active gameplay moments. By focusing on the periods around kill/death events, you're likely to capture the most relevant and active input patterns, potentially reducing the impact of long periods of inactivity.
Suggested Approach:
a. Implement the event-focused preprocessing.
b. Rerun your existing analyses (Poisson, Negative Binomial, etc.) on this preprocessed data.
c. If these still don't fit well, consider trying Zero-Inflated models or Hurdle models.
d. In parallel, consider applying some machine learning methods, particularly Random Forests or Gradient Boosting, as these can handle complex distributions without making strong assumptions about the underlying distribution.
e. For a deeper understanding of the temporal patterns, consider using RNNs or LSTMs if you have sufficient data.
Additional Considerations:

Feature Engineering: Create features that capture the "burstiness" of inputs, such as the time since last input or rolling averages of input frequency.
Segmentation: Consider analyzing different types of gameplay moments separately (e.g., combat vs. exploration).
Player Skill Level: If available, incorporating player skill level as a feature could provide additional insights.



By implementing the event-focused preprocessing and exploring these alternative modeling approaches, you're likely to gain more meaningful insights into how latency affects player performance during critical gameplay moments. This approach should help mitigate the issues caused by periods of inactivity and provide a more nuanced understanding of player behavior under different latency conditions.

My next steps in my words:
To test preprocessing to look at the just the data surrounding kills/deaths 
If that doesn't work with negative binomial or poisson I will try Zero inflated poisson, zero inflated negative binomial and hurdle model.
I believe step 1 and 2 are 2 different methods of getting the same results as they're both helping with all the zero values but will try both
If this doesn't work I'll just try some machine learning (random forest, gradient boosting, RNN, LSTM)

TODO:

        Also please update the boxplots as requested by Max/Daniel - Boxplots next to each other for affected vs non affected. Shaded/lines for one vs other. Take data from affected_players txt

        Relative measure? divide average number of kills from baseline to show deviation of kills/deaths

Normalisation:

        Normalizing the data for this type of event-focused analysis in an FPS game context is an excellent idea. It can help reduce biases and make comparisons across different players, events, or latency conditions more meaningful. Here are some methods you could consider for normalizing this data:

        Min-Max Normalization:
        This would scale all values to a range between 0 and 1.

        Min-Max Normalization FunctionClick to open code

        Z-Score Normalization:
        This would transform the data to have a mean of 0 and a standard deviation of 1.

        Z-Score Normalization FunctionClick to open code

        Normalization by Player:
        Since different players might have different baseline input frequencies, you might want to normalize within each player's data.

        Player-Specific Normalization FunctionClick to open code

        Event-Type Normalization:
        You might want to normalize separately for different event types (e.g., kills vs. deaths).

        Event-Type Normalization FunctionClick to open code

        Time-window Normalization:
        For each event, you could normalize the input frequencies relative to the start of the time window.

        Time-Window Normalization FunctionClick to open code
        When choosing a normalization method, consider:

        The specific questions you're trying to answer with your analysis.
        Whether you want to compare across players, event types, or time windows.
        The distribution of your data (some methods assume normal distribution).

        You might even want to apply multiple normalization techniques and compare the results. For example, you could first normalize by player to account for individual differences, then apply time-window normalization to each event.
        Remember to clearly document which normalization method(s) you've used when interpreting and presenting your results, as this will affect how the data should be understood.